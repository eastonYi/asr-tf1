dirs:
    exp: aishell
    train:
        scp: /mnt/lustre/xushuang2/easton/data/aishell/feats/feats.train-10k.scp
        trans: /mnt/lustre/xushuang2/easton/data/aishell/phone/train-10k.phone67
        tfdata: /mnt/lustre/xushuang2/easton/data/aishell/phone/tfdata/train-10k_xy
    untrain:
        scp: /mnt/lustre/xushuang2/easton/data/aishell/feats/feats.train-120k.scp
        trans: /mnt/lustre/xushuang2/easton/data/aishell/phone/train-120k.phone67
        tfdata: /mnt/lustre/xushuang2/easton/data/aishell/phone/tfdata/train-120k_xy
    dev:
        scp: /mnt/lustre/xushuang2/easton/data/aishell/feats/feats.dev-14k.scp
        trans: /mnt/lustre/xushuang2/easton/data/aishell/phone/dev-14k.phone67
        tfdata: /mnt/lustre/xushuang2/easton/data/aishell/phone/tfdata/dev-14k_xy
    test:
        scp: /mnt/lustre/xushuang2/easton/data/aishell/feats/feats.test-7k.scp
        trans: /mnt/lustre/xushuang2/easton/data/aishell/phone/test-7k.phone67
        tfdata: /mnt/lustre/xushuang2/easton/data/aishell/phone/tfdata/test-7k_xy
    text:
        data: /mnt/lustre/xushuang2/easton/data/aishell/phone/text-120k.phone67
        ngram: /mnt/lustre/xushuang2/easton/data/aishell/phone/text-120k.5gram
    type: scp
    vocab: /mnt/lustre/xushuang2/easton/data/aishell/phone/phones_67+1.vocab
    # checkpoint_G: /mnt/lustre/xushuang2/easton/projects/asr-tf1/exps/aishell/aishell_ctc2/no_norm_1k/checkpoint
    checkpoint_G: /mnt/lustre/xushuang2/easton/projects/asr-tf1/exps/aishell/aishell_ctc2/no_norm_5k/checkpoint/model-799
    # checkpoint: /mnt/lustre/xushuang2/easton/projects/asr-tf1/exps/aishell/aishell_ctc/merge_false_train-10k/checkpoint

data:
    featType: mfcc
    left_context: 0
    right_context: 0
    downsample: 1
    add_delta: False
    unit: word

EODM:
    ngram: 5
    top_k: 1000
    k: 1000

model:
    type: ctcModel_EODM
    encoder:
        type: conv2
        num_filters: 256
        num_blocks: 1
        hidden_size: 256
        num_fc: 3
    decoder:
        type: FC

# lr_type: constant_learning_rate
optimizer: adam
lr: 0.00001
warmup_steps: 2000
peak: 0.0002
decay_steps: 3000
# warmup_steps: 600
# peak: 0.0002
# decay_steps: 1000

dev_step: 100
decode_step: 600
save_step: 600
num_epochs: 999

gpus: '0,1,2,3'
# gpus: '0,1'
# gpus: '4,5,6,7'
# gpus: '4,6,7'
batch_size: 100
text_batch_size: 1
beam_size: 1
num_batch_tokens: 160000
bucket_boundaries: 403,432,465,500,537,578,629,706,905,1230

lambda_lm: 0.0
grad_clip_value: 10.0
grad_clip_norm: 10.0
grad_clip_global_norm: 10.0
